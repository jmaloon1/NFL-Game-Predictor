{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['team', 'win%', 'date'])\n",
    "\n",
    "output_file = 'z:\\python_projects\\aaa.exe'\n",
    "year = 2019\n",
    "URL = 'https://projects.fivethirtyeight.com/2019-nfl-predictions/games/?ex_cid=rrpromo'\n",
    "headers = {\"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}\n",
    "source = requests.get(URL, headers=headers)\n",
    "soup = BeautifulSoup(source.content, 'html.parser')\n",
    "\n",
    "# Finding year of predictions\n",
    "for timestamp in soup.findAll('div', attrs={'class': 'container'}):\n",
    "    for year in timestamp.findAll('div', attrs={'id': 'intro'}):\n",
    "        year = int(year.h1.get_text()[0:5])\n",
    "        \n",
    "# Gathering data\n",
    "for week in soup.findAll('section', attrs={'class': 'week'}):\n",
    "    if week.get_text()[5] == '5':\n",
    "        for date in week.findAll('div', attrs={'class': 'days'}):\n",
    "            for day in date.findAll('div', attrs={'class': 'day'}):  \n",
    "                for game in day.findAll('div', attrs={'class': 'game'}):\n",
    "                    \n",
    "                    # Finding date of each game  \n",
    "                    for h4 in day.findAll('h4', attrs={'class': 'h4'}):\n",
    "                        date_list.append(h4.get_text())\n",
    "                        date = h4.get_text()\n",
    "                        date = date.split(', ')[1]\n",
    "                        date = date[0:3] + date[4:6] + ' ' + str(year)\n",
    "                        date = datetime.strptime(date, '%b %d %Y')\n",
    "                        date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "\n",
    "                    # Finding data for each game\n",
    "                    for game_body in game.findAll('table', attrs={'class': 'game-body'}):           \n",
    "                        for num_teams, matchup in enumerate(game_body.findAll('tr', attrs={'class': 'tr'})):\n",
    "                            for name in matchup.findAll('td', attrs={'class': 'td text team'}):\n",
    "                                if num_teams == 0:\n",
    "                                    team = name.get_text().strip()\n",
    "                                else:\n",
    "                                    team = name.get_text().strip()\n",
    "                                    \n",
    "                            for win_percentage in matchup.findAll('td', attrs={'class': 'td number chance'}):\n",
    "                                if num_teams == 0:\n",
    "                                    prob = win_percentage.get_text()\n",
    "                                else:\n",
    "                                    prob = win_percentage.get_text()\n",
    "\n",
    "                            if num_teams ==  0:\n",
    "                                df = df.append(pd.Series([team, prob, date], \n",
    "                                               index=['team', 'win%', 'date']), ignore_index=True)\n",
    "                            else:\n",
    "                                df = df.append(pd.Series([team, prob, ''], \n",
    "                                               index=['team', 'win%', 'date']), ignore_index=True)\n",
    "                                \n",
    "\n",
    "#div = soup.findAll('section', attrs={'class': 'week'}).get_text()\n",
    "#div2 = soup.select('h3.days')#.week-group.week.days.daygames_wrap.game.game-bod')\n",
    "\n",
    "#print(div)\n",
    "#df.to_csv('z:\\\\Python Projects\\\\Score Predictor.csv', mode = 'a')\n",
    "from datetime import datetime\n",
    "datetime.strptime('Oct', '%b')\n",
    "df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = ['Open', 'odds','Westgate Superbook','MGM Mirage', 'betMGM', \n",
    "         'William Hill', 'CG Technology', 'Circa Sports','Stations']\n",
    "book = pd.DataFrame(columns=names)\n",
    "output_file = 'z:\\python_projects\\aaa.exe'\n",
    "temp_away_list = []\n",
    "temp_home_list = []\n",
    "teams = []\n",
    "URL = 'http://www.vegasinsider.com/nfl/odds/las-vegas/money/'\n",
    "headers = {\"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}\n",
    "source = requests.get(URL, headers=headers)\n",
    "soup = BeautifulSoup(source.content, 'html.parser')\n",
    "\n",
    "for gameboard in soup.findAll('table', attrs={'class': 'viBodyContainerTble'}):\n",
    "    for games in gameboard.findAll('td', attrs={'class': 'viBodyBorderNorm'}):\n",
    "        for team_names in games.findAll('td', attrs={'class': 'viCellBg1 cellTextNorm cellBorderL1'}):\n",
    "            for team_name in team_names.findAll('a', attrs={'class': 'tabletext'}):\n",
    "                teams.append(team_name.get_text())\n",
    "\n",
    "        for team_names in games.findAll('td', attrs={'class': 'viCellBg2 cellTextNorm cellBorderL1'}):\n",
    "            for team_name in team_names.findAll('a', attrs={'class': 'tabletext'}):\n",
    "                teams.append(team_name.get_text())\n",
    "\n",
    "        for num, spread in enumerate(games.findAll('td', attrs={'class': ['viCellBg1 cellTextNorm cellBorderL1 center_text nowrap',\n",
    "                                                                          'viCellBg1 cellTextHot cellBorderL1 center_text nowrap']})):           \n",
    "            spread_text = spread.get_text().strip()\n",
    "            \n",
    "            if spread_text == '' or spread_text == 'XXXX':\n",
    "                away_spread = np.nan\n",
    "                home_spread = np.nan\n",
    "            elif(spread_text[4] == '+' or spread_text[4] == '-'):\n",
    "                away_spread = spread_text[0:4]\n",
    "                home_spread = spread_text[4:]\n",
    "            else:\n",
    "                away_spread = spread_text[0:5]\n",
    "                home_spread = spread_text[5:]\n",
    "\n",
    "            temp_away_list.append(away_spread)\n",
    "            temp_home_list.append(home_spread)\n",
    "            if len(temp_home_list) == 9:\n",
    "                book = book.append(pd.Series(temp_away_list, index=names), ignore_index=True)\n",
    "                book = book.append(pd.Series(temp_home_list, index=names), ignore_index=True)\n",
    "                temp_away_list = []\n",
    "                temp_home_list = []\n",
    "                    \n",
    "        for num, spread in enumerate(games.findAll('td', attrs={'class': ['viCellBg2 cellTextNorm cellBorderL1 center_text nowrap',\n",
    "                                                                          'viCellBg2 cellTextHot cellBorderL1 center_text nowrap']})):           \n",
    "            spread_text = spread.get_text().strip()\n",
    "\n",
    "            if spread_text == '' or spread_text == 'XXXX':\n",
    "                away_spread = np.nan\n",
    "                home_spread = np.nan\n",
    "            elif(spread_text[4] == '+' or spread_text[4] == '-'):\n",
    "                away_spread = spread_text[0:4]\n",
    "                home_spread = spread_text[4:]\n",
    "            else:\n",
    "                away_spread = spread_text[0:5]\n",
    "                home_spread = spread_text[5:]\n",
    "            temp_away_list.append(away_spread)\n",
    "            temp_home_list.append(home_spread)\n",
    "\n",
    "            if len(temp_home_list) == 9:\n",
    "                book = book.append(pd.Series(temp_away_list, index=names), ignore_index=True)\n",
    "                book = book.append(pd.Series(temp_home_list, index=names), ignore_index=True)\n",
    "                temp_away_list = []\n",
    "                temp_home_list = []\n",
    "\n",
    "                    #print(away_spread)\n",
    "                    #print(home_spread)\n",
    "book['team'] = teams[0:len(book.index)]\n",
    "book.set_index('team', inplace=True)\n",
    "\n",
    "book.dropna(how='all', inplace=True)\n",
    "book = book.reindex(index=df.team)\n",
    "book['team'] = book.index\n",
    "book.replace(np.nan, '', inplace=True)\n",
    "odds = book[['odds', 'team']]\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spreadsheet = pd.merge(odds, df)\n",
    "spreadsheet = spreadsheet[['date', 'team', 'win%', 'odds']]\n",
    "spreadsheet.set_index('date')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
